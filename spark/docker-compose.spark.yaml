version: '1.0.0'
name: data_platform

x-spark-common: &spark-common
  # image: bitnami/spark:latest
  build: 
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./spark.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  
  environment: &spark-common-env
    SPARK_CONF_DIR: /opt/bitnami/spark/conf
    AWS_ACCESS_KEY_ID: adminMinio
    AWS_SECRET_ACCESS_KEY: adminMinio
    # SPARK_CLASSPATH: /opt/bitnami/spark/jars/*:/opt/bitnami/spark/additionaljars/*
    #SPARK_EXTRA_CLASSPATH: /opt/bitnami/spark/additionaljars/*
    # HADOOP_HOME: "/opt/bitnami/spark"
    # HADOOP_CONF_DIR: "/opt/bitnami/spark/conf"
    # HADOOP_OPTIONAL_TOOLS: "hadoop-aws"
    # # SPARK_SUBMIT_OPTS: "-Dhadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
    # SPARK_SUBMIT_OPTS: "-Dhadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dhadoop.fs.s3a.endpoint=http://minio:9000 -Dhadoop.fs.s3a.path.style.access=true"
  networks:
    - data_platform_network

services:
  spark-master:
    <<: *spark-common
    hostname: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8082:8080"
      - "7077:7077"

  spark-worker-1:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      <<: *spark-common-env  
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 3g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      <<: *spark-common-env   
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 3g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-thrift:
    <<: *spark-common
    command: /opt/bitnami/spark/sbin/start-thriftserver.sh \
      --master spark://spark-master:7077 \
      --conf spark.sql.catalogImplementation=hive \
      --conf spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension \
      --packages io.delta:delta-spark_2.12:3.3.0,io.delta:delta-core_2.12:3.3.0
    depends_on:
      - spark-master
    ports:
      - "10000:10000"
    environment:
      <<: *spark-common-env
      SPARK_MODE: thriftserver


networks:
  data_platform_network:
    external: true  