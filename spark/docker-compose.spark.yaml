version: '1.0.0'
name: data_platform

x-spark-common: &spark-common
  # image: bitnami/spark:latest
  build: 
    context: .
    dockerfile: Dockerfile
  volumes:
    - ./spark.conf:/opt/bitnami/spark/conf/spark-defaults.conf
  
  environment: &spark-common-env
    SPARK_CONF_DIR: /opt/bitnami/spark/conf
    AWS_ACCESS_KEY_ID: adminMinio
    AWS_SECRET_ACCESS_KEY: adminMinio
    # SPARK_CLASSPATH: /opt/bitnami/spark/jars/*:/opt/bitnami/spark/additionaljars/*
    #SPARK_EXTRA_CLASSPATH: /opt/bitnami/spark/additionaljars/*
    # HADOOP_HOME: "/opt/bitnami/spark"
    # HADOOP_CONF_DIR: "/opt/bitnami/spark/conf"
    # HADOOP_OPTIONAL_TOOLS: "hadoop-aws"
    # # SPARK_SUBMIT_OPTS: "-Dhadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem"
    # SPARK_SUBMIT_OPTS: "-Dhadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem -Dhadoop.fs.s3a.endpoint=http://minio:9000 -Dhadoop.fs.s3a.path.style.access=true"
  networks:
    - data_platform_network

services:
  spark-master:
    <<: *spark-common
    hostname: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "8082:8080"
      - "7077:7077"

  spark-worker-1:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      <<: *spark-common-env  
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 3g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      <<: *spark-common-env   
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 3g
      SPARK_MASTER_URL: spark://spark-master:7077

  # spark-thrift:
  #   <<: *spark-common
  #   command: /opt/bitnami/spark/sbin/start-thriftserver.sh --master spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   ports:
  #     - "10000:10000"
  #   environment:
  #     <<: *spark-common-env
  #     SPARK_MODE: thriftserver
  #   #   SPARK_DAEMON_JAVA_OPTS: -Dspark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension -Dspark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog
  #   #   SPARK_CLASSPATH: /opt/bitnami/spark/jars/*:/opt/bitnami/spark/additionaljars/*
  #   # volumes:
  #   #   - ./jars:/opt/bitnami/spark/additionaljars


networks:
  data_platform_network:
    external: true  